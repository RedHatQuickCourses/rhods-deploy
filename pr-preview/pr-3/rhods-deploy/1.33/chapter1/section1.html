<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Section 1: Concepts :: Deploying Machine Learning Models with Red Hat OpenShift Data Science</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="section2.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Deploying Machine Learning Models with Red Hat OpenShift Data Science</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/rhods-deploy/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="rhods-deploy" data-version="1.33">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Deploying Machine Learning Models with Red Hat OpenShift Data Science</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Model Serving in RHODS</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section1.html">Section 1: Concepts</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section2.html">Section 2</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Section 3</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Chapter 2</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/section1.html">Section 1</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter3/index.html">Chapter 3</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section1.html">Section 1</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter3/section2.html">Section 2</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../appendix/appendix.html">Appendix A</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Deploying Machine Learning Models with Red Hat OpenShift Data Science</span>
    <span class="version">1.33</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Deploying Machine Learning Models with Red Hat OpenShift Data Science</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.33</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Deploying Machine Learning Models with Red Hat OpenShift Data Science</a></li>
    <li><a href="index.html">Model Serving in RHODS</a></li>
    <li><a href="section1.html">Section 1: Concepts</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Section 1: Concepts</h1>
<div class="sect1">
<h2 id="_model_serving"><a class="anchor" href="#_model_serving"></a>Model Serving</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Machine learning model serving refers to the process of deploying, managing, and exposing machine learning models for inference, allowing them to make predictions or classifications on new, unseen data. Once a machine learning model has been trained and fine-tuned on a specific task, serving involves making the model available for real-time or batch predictions in a production environment. This is a crucial step in the machine learning lifecycle, as it enables the integration of models into applications, systems, or services, allowing them to deliver value in real-world scenarios.</p>
</div>
<div class="paragraph">
<p>Key aspects of machine learning model serving include:</p>
</div>
<div class="paragraph">
<p><strong>Deployment</strong>:</p>
</div>
<div class="paragraph">
<p>After training, a machine learning model needs to be deployed to an environment where it can be accessed for inference. This can be on-premises, in the cloud, or on edge devices, depending on the application&#8217;s requirements.</p>
</div>
<div class="paragraph">
<p><strong>Scalability</strong>:</p>
</div>
<div class="paragraph">
<p>Model serving systems need to be scalable to handle varying levels of demand. This involves ensuring that the infrastructure can handle increased load and that the serving architecture is designed for efficiency and resource optimization.</p>
</div>
<div class="paragraph">
<p><strong>Real-time Inference</strong>:</p>
</div>
<div class="paragraph">
<p>Many applications require real-time predictions, where the model responds to input data quickly. This is common in scenarios such as fraud detection, image recognition, and natural language processing.</p>
</div>
<div class="paragraph">
<p><strong>Batch Inference</strong>:</p>
</div>
<div class="paragraph">
<p>In some cases, models need to process large batches of data in a more efficient, non-real-time manner. This is common in scenarios like data preprocessing, analytics, and large-scale data transformations.</p>
</div>
<div class="paragraph">
<p><strong>Monitoring and Logging</strong>:</p>
</div>
<div class="paragraph">
<p>Model serving systems often include monitoring and logging functionalities to track the performance and health of deployed models. This includes metrics such as response time, error rates, and resource utilization.</p>
</div>
<div class="paragraph">
<p><strong>Versioning:</strong></p>
</div>
<div class="paragraph">
<p>It&#8217;s important to manage and version different iterations of a model. This allows for easy rollback in case of issues and facilitates A/B testing when deploying new models.</p>
</div>
<div class="paragraph">
<p><strong>Security</strong>:</p>
</div>
<div class="paragraph">
<p>Ensuring the security of the deployed models is crucial. This involves controlling access to the models, encrypting communications, and protecting against potential attacks or adversarial inputs.</p>
</div>
<div class="paragraph">
<p><strong>Integration</strong>:</p>
</div>
<div class="paragraph">
<p>Model serving systems need to integrate seamlessly with other components of the application or system architecture. This often involves using APIs (Application Programming Interfaces) to facilitate communication between the model and the rest of the application.
Popular tools and frameworks for model serving include TensorFlow Serving, Flask, FastAPI, Amazon SageMaker, and TensorFlow Extended (TFX), among others. These tools provide solutions for deploying, managing, and scaling machine learning models in production environments.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_model_server_runtime"><a class="anchor" href="#_model_server_runtime"></a>Model Server Runtime</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The terms "model server runtime" and "inference engine" are closely related in the context of deploying and serving machine learning models, but they refer to different components of the overall system.</p>
</div>
<div class="paragraph">
<p>A model server runtime is the execution environment or platform where a trained machine learning model runs to make predictions or inferences. It is responsible for loading the model into memory, handling requests from clients, performing the inference, and returning the results. The runtime provides the necessary infrastructure for hosting the model, managing resources, and ensuring that the model&#8217;s predictions can be efficiently served in a production environment.</p>
</div>
<div class="paragraph">
<p>The model server runtime can be a part of a larger deployment framework or service that includes features such as scalability, versioning, monitoring, and security. Examples of model server runtimes include TensorFlow Serving, TorchServe, and ONNX Runtime. These runtimes support the deployment of models trained using popular machine learning frameworks and provide a standardized way to serve predictions over APIs (Application Programming Interfaces).</p>
</div>
<div class="sect2">
<h3 id="_inference_engine"><a class="anchor" href="#_inference_engine"></a>Inference Engine:</h3>
<div class="paragraph">
<p>An inference engine is a component responsible for executing the forward pass of a machine learning model to generate predictions based on input data. It is a crucial part of the model server runtime and is specifically designed for performing inference tasks efficiently. The inference engine takes care of optimizations, such as hardware acceleration and parallelization, to ensure that predictions are made quickly and with minimal resource utilization.</p>
</div>
<div class="paragraph">
<p>The inference engine may be integrated into the model server runtime or work alongside it, depending on the specific architecture. For example, TensorFlow Serving incorporates TensorFlow&#8217;s inference engine, and ONNX Runtime serves as both a runtime and an inference engine for models in the Open Neural Network Exchange (ONNX) format.</p>
</div>
<div class="paragraph">
<p><strong>Relationship</strong>:
In summary, the model server runtime provides the overall environment for hosting and managing machine learning models in production, while the inference engine is responsible for the actual computation of predictions during inference. The two work together to deliver a scalable, efficient, and reliable solution for serving machine learning models in real-world applications. The choice of model server runtime and inference engine depends on factors such as the machine learning framework used, deployment requirements, and the specific optimizations needed for the target hardware.</p>
</div>
</div>
<div class="sect2">
<h3 id="_unravel_the_runtime"><a class="anchor" href="#_unravel_the_runtime"></a>Unravel The Runtime</h3>
<div class="paragraph">
<p>When deploying machine learning models, we need to deploy a container that serves a <strong>Runtime</strong> and uses a <strong>Model</strong> to perform predictions, consider the following example:</p>
</div>
<div class="sect3">
<h4 id="_train_a_model"><a class="anchor" href="#_train_a_model"></a>Train a model</h4>
<div class="paragraph">
<p>Using a RHODS instance, let us train and deploy an example.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In a data science project, create a `Standard Data Science`workbench.
Then, open the workbench to go to the JuypterLab interface.</p>
<div class="imageblock">
<div class="content">
<img src="_images/workbench_options.png" alt="Workbench Options">
</div>
</div>
</li>
<li>
<p>Import following repostitory to the workbench: <a href="https://github.com/RedHatQuickCourses/rhods-qc-apps.git" class="bare">https://github.com/RedHatQuickCourses/rhods-qc-apps.git</a></p>
</li>
<li>
<p>Make sure that the workbench environment serves the required python packages for the notebook to run, for this to happen, open a terminal and run the following command to verify that the packages are already installed:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">  pip install -r /opt/app-root/src/rhods-qc-apps/4.rhods-deploy/chapter1/requirements.txt</code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/terminal-install.png" alt="terminal install">
</div>
</div>
</li>
<li>
<p>Open the notebook <strong>purchase-amount</strong> from the <strong>rhods-qc-apps/4.rhods-deploy/chapter1/purchase-amount.ipynb</strong> directory:</p>
<div class="imageblock">
<div class="content">
<img src="_images/purchase-amount-notebook.png" alt="purchase-amount notebook">
</div>
</div>
</li>
<li>
<p>Run the notebook, and notice the creation of a new file in your environment, the <code>mymodel.pkl</code></p>
<div class="imageblock">
<div class="content">
<img src="_images/mymodel-pkl.png" alt="Model file export">
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>There are different formats and libraries to export the model, in this case we are using pickle. Other common formats are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Protobuf</p>
</li>
<li>
<p>MLeap</p>
</li>
<li>
<p>H5</p>
</li>
<li>
<p>ONNX</p>
</li>
<li>
<p>PMML</p>
</li>
<li>
<p>Torch</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The use of either of those formats depend on the target server runtime, some of them are proven to be more eficient than others for certain type of training algorithms and model sizes.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="_use_the_model_in_another_notebook"><a class="anchor" href="#_use_the_model_in_another_notebook"></a>Use the model in another notebook</h5>
<div class="paragraph">
<p>The model can be deserialized in another notebook, and used to generate a prediction:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open the notebook <strong>use-purchase-amount</strong> from the <strong>rhods-qc-apps/4.rhods-deploy/chapter1/use-purchase-amount.ipynb</strong> directory:</p>
<div class="imageblock">
<div class="content">
<img src="_images/use-purchase-amount-notebook.png" alt="use-purchase-amount notebook create">
</div>
</div>
</li>
<li>
<p>Run the <strong>use-purchase-amount</strong> notebook and notice the result:</p>
<div class="ulist">
<ul>
<li>
<p>You can get the same result without training the model again.</p>
</li>
<li>
<p>You are not training the model in the <strong>user-purchase-amount</strong> notebook, you are re-using the output from the training notebook, and using the generated model to generate an inference.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>At this moment the model can be exported and imported in other projects for its use. Normally there will be an S3 bucket or a model registry to store models and versions of such models, and instead of manually exporting the model, there would be pipelines making the model available.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_use_the_model_in_a_container"><a class="anchor" href="#_use_the_model_in_a_container"></a>Use the Model in a Container</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For this section, you will need postman (or docker) to create an image, and a registry to upload the resulting image.</p>
</div>
<div class="sect2">
<h3 id="_web_application_that_uses_the_model"><a class="anchor" href="#_web_application_that_uses_the_model"></a>web application that uses the model</h3>
<div class="paragraph">
<p>The pickle model that we previously exported can be used in a Flask application. In this section we present an example Flask application that uses the model.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Although we are actually serving a model with Flask in the exercise, Flask is not considered part of the Model Serving feature. This example represents one way in which some customers decide to embed their models in containers, although RHODS provides for mechanisms that can make this process of serving a model a simpler process, when provided with the proper model formats.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In your computer, create a new directory to save the source code of the web application.
Navigate to that directory.</p>
</li>
<li>
<p>Download the <code>mymodel.pkl</code> file from JupyterLab into this directory.</p>
</li>
<li>
<p>Open the directory with a python IDE, then create a python script named <code>app.py</code> with the following code:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python[app.py] hljs" data-lang="python[app.py]">from flask import Flask, request
import pickle

app = Flask(__name__)
# Load model
with open('mymodel.pkl', 'rb') as f:
    model = pickle.load(f)

model_name = "Time to purchase amount predictor"
model_file = 'model.plk'
version = "v1.0.0"


@app.route('/info', methods=['GET'])
def info():
    """Return model information, version how to call"""
    result = {}

    result["name"] = model_name
    result["version"] = version

    return result


@app.route('/health', methods=['GET'])
def health():
    """REturn service health"""
    return 'ok'


@app.route('/predict', methods=['POST'])
def predict():
    feature_dict = request.get_json()
    if not feature_dict:
        return {
            'error': 'Body is empty.'
        }, 500

    try:
        return {
            'status': 200,
            'prediction': int(model(feature_dict['time']))
        }
    except ValueError as e:
        return {'error': str(e).split('\n')[-1].strip()}, 500


if __name__ == '__main__':
    app.run(host='0.0.0.0')</code></pre>
</div>
</div>
</li>
<li>
<p>Create a <code>requirements.txt</code> to describe the python dependencies to install on container startup:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-[requirements.txt] hljs" data-lang="[requirements.txt]">click==8.0.3
cycler==0.11.0
Flask==2.0.2
fonttools==4.28.5
gunicorn==20.1.0
itsdangerous==2.0.1
Jinja2==3.0.3
kiwisolver==1.3.2
MarkupSafe==2.0.1
matplotlib==3.5.1
numpy==1.22.0
packaging==21.3
pandas==1.3.5
Pillow==9.0.0
pyparsing==3.0.6
python-dateutil==2.8.2
pytz==2021.3
scikit-learn==1.0.2
scipy==1.7.3
six==1.16.0
sklearn==0.0
threadpoolctl==3.0.0
Werkzeug==2.0.2</code></pre>
</div>
</div>
</li>
<li>
<p>Create a <code>Containerfile</code> to build an image with the Flask application:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-docker[containerfile] hljs" data-lang="docker[containerfile]"># Base image
FROM python:3.9

# Set working directory
WORKDIR /app

# Copy files
COPY app.py /app <i class="conum" data-value="1"></i><b>(1)</b>
COPY requirements.txt /app <i class="conum" data-value="2"></i><b>(2)</b>
COPY mymodel.pkl /app <i class="conum" data-value="3"></i><b>(3)</b>

# Install dependencies
RUN pip install -r requirements.txt

# Run the application
EXPOSE 8000
ENTRYPOINT ["gunicorn", "-b", "0.0.0.0:8000", "--access-logfile", "-", "--error-logfile", "-", "--timeout", "120"]
CMD ["app:app"]</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The python application source code</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The list of packages to install</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The model</td>
</tr>
</table>
</div>
</li>
<li>
<p>Build and push the image to an image registry</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">podman login quay.io
podman build -t purchase-predictor:1.0 .
podman tag purchase-predictor:1.0 quay.io/user_name/purchase-predictor:1.0
podman push quay.io/user_name/purchase-predictor:1.0</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the model image to <strong>OpenShift</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">oc login api.cluster.example.com:6443
oc new-project model-deploy
oc new-app --name purchase-predictor quay.io/user_name/purchase-predictor:1.0
oc expose service purchase-predictor</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Now we can use the Flask application with some commands such as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">curl http://purchase-predictor-model-deploy.apps.cluster.example.com/health
ok%
curl http://purchase-predictor-model-deploy.apps.cluster.example.com/info
{"name":"Time to purchase amount predictor","version":"v1.0.0"}
curl -d '{"time":4}' -H "Content-Type: application/json" -X POST http://purchase-predictor-model-deploy.apps.cluster.example.com/predict
{"prediction":34,"status":200}</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In this section we have manually:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Developed an application that uses the model</p>
</li>
<li>
<p>Built an image with such application</p>
</li>
<li>
<p>Push the image to a registry</p>
</li>
<li>
<p>Deployed the containerized application in OpenShift</p>
</li>
<li>
<p>Exposed the application&#8217;s endpoint in OpenShift by creating a route</p>
</li>
<li>
<p>Consumed the model through the application&#8217;s REST API to request a prediction</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>There are automated and faster ways to perform these steps. In the following sections, we will learn about runtimes that only require you to provide a model, and they automatically provision an inference service for you.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_model_servers_and_container_builds"><a class="anchor" href="#_model_servers_and_container_builds"></a>Model Servers And Container Builds</h3>
<div class="paragraph">
<p>As you can see in the previous example, we manually created a Model Server by sending the model to an image that can interpret the model and expose it for consumtion. In our example we used Flask.</p>
</div>
<div class="paragraph">
<p>OpenShift offer integrations to pre-configured images that can receive an specific model format.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://onnx.ai">ONNX</a>: An open standard for machine learning interoperability.</p>
</li>
<li>
<p><a href="https://docs.openvino.ai/latest/openvino_ir.html">OpenVino IR</a>: The proprietary model format of OpenVINO, the model serving runtime used in OpenShift Data Science.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In order to leverage the beneffits of such configurations you will:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Export the model in a format compatible with one of the available RHODS runtimes.</p>
</li>
<li>
<p>Upload the model to an S3</p>
</li>
<li>
<p>Create a Data Connection to the S3 containing the model</p>
</li>
<li>
<p>Create or use one of the available serving runtimes in a Model Server configuration that specifies the size and resources to use while setting up an inference engine.</p>
</li>
<li>
<p>Start a model server instance to publish your model for consumtion</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>While publishing this model server instance, the configurations will allow you to define how applications securelly connect to your model server to request for predictions, and the resources that it can provide.</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">Model Serving in RHODS</a></span>
  <span class="next"><a href="section2.html">Section 2</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
