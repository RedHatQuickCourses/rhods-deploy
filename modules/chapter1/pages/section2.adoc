= OpenVINO Model Serving

In this section we will work in an exercise to deploy a model to an OpenVINO Serving Runtime.

== Prepare MinIO

https://min.io[MinIO] is a high-performance, S3 compatible object store. It is built for large scale AI/ML, data lake and database workloads. It is software-defined and runs on any cloud or on-premises infrastructure.

We will need an S3 solution to share the model from training to deploy, in this exercise we will prepare MinIO to be such S3 solution.

. In OpenShift, create a new namespace with the name **object-datastore**.
+
[source,console]
----
$ oc new-project object-datastore
----

. Run the following yaml to install MinIO:
+
[source,console]
----
$ oc apply -f https://raw.githubusercontent.com/RedHatQuickCourses/rhods-qc-apps/main/4.rhods-deploy/chapter2/minio.yml -n object-datastore
----

. Get the route to the MinIO dashboard.
+
[source,console]
----
$ oc get routes -n object-datastore | grep minio-ui | awk '{print $2}'
----
+
[INFO]
====
Use this route to navigate to the S3 dashboard using a browser. With the browser, you will be able to create buckets, upload files, and navigate the S3 contents.
====

. Get the route to the MinIO API.
+
[source,console]
----
$ oc get routes -n object-datastore | grep minio-api | awk '{print $2}'
----
+
[INFO]
====
Use this route as the S3 API endpoint. Basically, this is the URL that we will use when creating a data connection to the S3 in RHOAI.
====

== Training The Model
We will use the iris dataset model for this excercise.

. Using a JupyterLab workbench at RHOAI, import the repository: https://github.com/RedHatQuickCourses/rhods-qc-apps.git
+
[TIP]
====
It is recommended to use a workbench that was created with the **Standard Data Science** Notebook image.
====

. Make sure that the workbench environment serves the required python packages for the notebook to run, for this to happen, open a terminal and run the following command to verify that the packages are already installed:
+
[source,console]
----
$ pip install -r /opt/app-root/src/rhods-qc-apps/4.rhods-deploy/chapter2/requirements.txt
----

[TIP]
====
You might also want to execute the preceding command in the notebook kernel by using the `%pip` syntax in the notebook. 
Alternatively, you can create a custom notebook image that includes the `skl2onnx` package.
====
. Open and run the notebook **iris_to_onnx** from **rhods-qc-apps/4.rhods-deploy/chapter2** directory
+
image::iris_training_onnx.png[iris training to onnx format]
+
[NOTE]
====
Converting a model to ONNX format depends on the library that you use to create the model.
In this case, the model is created with Scikit-Learn, so you must use the https://onnx.ai/sklearn-onnx/[sklearn-onnx] library to perform the conversion.

To convert from PyTorch, see https://pytorch.org/tutorials/beginner/onnx/intro_onnx.html[Introduction to ONNX in the PyTorch docs].

To convert from TensorFlow, use the https://github.com/onnx/tensorflow-onnx[tf2onnx] library.
====

. Observe that a file has been created: `rf_iris.onnx`, download this file to your computer, so that we can upload it to S3.
+
image::iris-download.png[iris model download]

. Upload the file `rf_iris.onnx` to a bucket named **models**, with a path **iris** in your S3. The username is *minio* and the password is *minio123*.
+
image::iris-s3-upload.png[iris model s3 upload]
+
[IMPORTANT]
====
Make sure to create a new path in your bucket, and upload to such path, not to root. Later, when requesting to deploy a model to the **Model Server**, you will be required to provide a path inside your bucket.
====

== Create A Data Connection

. In the RHOAI dashboard, create a project named **iris-project**.

. In the **Data Connections** section, create a Data Connection to your S3.
+
image::add-minio-iris-data-connection.png[Add iris data connection from minio]
+
[IMPORTANT]
====
- The credentials (Access Key/Secret Key) are `minio`/`minio123`.
- Make sure to use the API route, not the UI route (`oc get routes -n object-datastore | grep minio-api | awk '{print $2}'`).
- The region is not important when using MinIO, this is a property that has effects when using AWS S3.
However, you must enter a non-empty value to prevent problems with model serving.
- Mind typos for the bucket name.
- You don't have to select a workbench to attach this data connection to.
====


== Using `boto3`

Although the previous section indicates that you should manually download the `rf_iris.onnx` file to your computer and upload it to S3, you can also upload your model directly from your notebook or Python file, by using the `boto3` library.
To use this approach, you must:

* Have the `boto3` library installed in your workbench (most of the RHOAI notebook images include this library).
* Attach your data connection to the workbench.

After training the model, you can upload the file as the following example demostrates:

[source,python]
----
import os
import boto3

source_path = "model.onnx"
s3_destination_path = "models/model.onnx"

key_id = os.getenv("AWS_ACCESS_KEY_ID")
secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
endpoint = os.getenv("AWS_S3_ENDPOINT")
bucket_name = os.getenv("AWS_S3_BUCKET")

s3 = boto3.client(
   "s3",
   aws_access_key_id=key_id,
   aws_secret_access_key=secret_key,
   endpoint_url=endpoint,
   use_ssl=True)

s3.upload_file(source_path, bucket_name, Key=s3_destination_path)
----

[NOTE]
====
You can also use the `boto3` library to download data.
This can be helpful in the data collection stage, for example for gathering data files from S3.

[source,python]
----
s3_data_path = "dataset.csv"
s3.download_file(bucket_name, s3_data_path, "my/local/path/dataset.csv")
----
====

== Create a Model Server

. In the **Models and model servers** section, add a server.
+
image::add-server-button.png[add server]

. Fill the form with the following values:
+
--
* Server name: `iris-model-server`.
* Serving runtime: `OpenVINO Model Server`.
* Select the checkboxes to expose the models through an external route, and to enable token authentication.
Enter `iris-serviceaccount` as the service account name.
--
+
image::add-server-form-example.png[Add Server Form]
+
[IMPORTANT]
====
The model server you are creating works as a template for deploying models. As you can see, we have not specified the model that we will deploy, or the data connection from where that model will be retrieved, in this form we are specifying the resources, constraints, and engine that will define the engine where the model will be deployed later.
It is important to pay special attention to the following characteristics:

- **Serving Runtime**: By default we have _OpenVINO_ and _OpenVINO with GPU_. The important aspects when defining these runtimes are: The framework that is capable of reading models in a given format, and weather such platform supports using GPUs. The use of GPUs allow for complex and lengthy computations to be delivered faster, as there are huge models that require a good amount of power to calculate, based on the given parameters a prediction.

- **Number of replicas to deploy**: Planning for expected performance and number of expected requests is essential for this part of the form. Here we select if we will load balance a given request between multiple container replicas.

- **Model Server Size**: In this part of the form we define the resources assigned to each model server container. You can create and select a pre-defined size from the dropdown, or you can select _custom_, in which case, new fields will be displayed to request the processing and memory power to be assigned to your containers.
+
image::model-server-size.png[model server size]

- **Model Route**: There are models that can be consumed only from other containers inside the same OpenShift cluster, here we have the ability to not make this server available to entities outside our cluster, or to instruct the model server configuration to assign an external route. When we don't expose the model externally through a route, click on the Internal Service link in the Inference endpoint section:
+
image::figure14_0.png[Inference endpoint]
+
A popup will display the address for the gRPC and the REST URLs:
+
image::figure15_0.png[Endpoint URLs] 

- **Token authorization**: In this part of the form we have a helper checkmark to add authorization to a service account that will be created with access to our model server. Only API requests that present a token that has access to the given service account will be able to run the inference service.
====

. After clicking the **Add** button at the bottom of the form, you will be able to see a new **Model Server** configuration in your project, you can click the **Tokens** column, which will make visible the tokens that you can share with the applications that will consume the inference API.
+
image::model-server-with-token.png[Model Server with token]

== Deploy The Model

. At the right side of the **Model Server**, we can find the **Deploy Model** button, let's click the **Deploy Model** button, to start filling the **Deploy Model** form:
+
image::deploy-model-button.png[Deploy Model button]

. Fill the **Deploy Model** form.
+
--
* Model name: `iris-model`
* Model framework: `onnx - 1`
* Model location data connection: `iris-data-connection`
* Model location path: `iris`
--
+
image::deploy-model-form.png[Deploy Model form]

. After clicking the **Add** button at the bottom of the form, you will be able to see a new entry at the **Deployed models** column for your **Model Server**, clicking in the column will eventually show a check mark under the **Status** column:
+
image::deploy-model-success.png[Deploy model success]

. Observe and monitor the assets created in your OpenShift **iris-project** namespace.
+
[source,console]
----
$ oc get routes -n iris-project
$ oc get secrets -n iris-project | grep iris-model
$ oc get events -n iris-project
----
+
image::iris-project-events.png[Iris project events]
+
[TIP]
====
Deploying a **Model Server** triggers a **ReplicaSet** with **ModelMesh**, which attach your model to the inference runtime, and exposes it through a route. Also, notice the creation of a secret with your token.
====

== Test The Model

Now that the model is ready to use, we can make an inference using the REST API

. Assign the route to an environment variable in your local machine, so that we can use it in our curl commands.
+
[source,console]
----
$ export IRIS_ROUTE=https://$(oc get routes -n iris-project | grep iris-model | awk '{print $2}')
----

. Assign an authentication token to an environment variable in your local machine.
+
[source,console]
----
$ export TOKEN=$(oc whoami -t)
----

. Request an inference with the REST API.
+
[source,console]
----
$ curl -H "Authorization: Bearer $TOKEN" $IRIS_ROUTE/v2/models/iris-model/infer \
 -X POST \
 --data '{"inputs" : [{"name" : "X","shape" : [ 1, 4 ],"datatype" : "FP32","data" : [ 3, 4, 3, 2 ]}],"outputs" : [{"name" : "output0"}]}'
----

The result of using the inference service looks like the following output:
```json
{"model_name":"iris-model__isvc-590b5324f9","model_version":"1","outputs":[{"name":"label","datatype":"INT64","shape":[1],"data":[1]},{"name":"scores","datatype":"FP32","shape":[1,3],"data":[4.851966,3.1275764,3.4580243]}]}
```

=== Model Serving Request Body

As you tested with the preceding `curl` command, to make HTTP requests to a deployed model you must use a specific request body format.
The basic format of the input data is as follows:

[subs=+quotes]
----
{
  "inputs": [{
    "name" : "input", <1>
    "shape" : [2,3], <2>
    "datatype"  : "INT64", <3>
    "data" : [[34, 54, 65], [4, 12, 21]] <4>
  }]
}
----
<1> The name of the input tensor.
The data scientist that creates the model must provide you with this value.
<2> The shape of the input tensor.
<3> The https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#tensor-data-types[data type] of the input tensor.
<4> The tensor contents provided as a JSON array.

The API supports additional parameters.
For a complete list, refer to the https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference-request-json-object[Kserve Predict Protocol docs].

To make a request in Python, you can use the `requests` library, as the following example shows:

[source,python]
----
import requests

input_data = [-0.15384616, -0.9909186]

# You must adjust this path or read it from an environment variable
INFERENCE_ENDPOINT = "https://my-model.apps.my-cluster.example.com/v2/models/my-model/infer"

# Build the request body
payload = {
    "inputs": [
        {
            "name": "dense_input",
            "shape": [1, 2],
            "datatype": "FP32",
            "data": input_data
        }
    ]
}

# Send the POST request
response = requests.post(INFERENCE_ENDPOINT, json=payload)

# Parse the JSON response
result = response.json()

# Print predicted values
print(result['outputs'][0]['data'])
----
