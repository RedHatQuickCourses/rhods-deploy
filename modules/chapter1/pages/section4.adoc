= KServe - Model Serving

RHOAI has two main components for serving 

* KServe Serving for single-model serving
* KServe ModelMesh for multi-model serving

The two components are similar in that they retrieve models from S3 buckets and expose these models via an inferencing endpoint.
However there are significant differences in how they work and the use cases they serve. 

== Model Mesh
In the multi-model approach serving pods can run one or more models simulateneously from the same pod. 
As requests for a particular model are received the ModelMesh routing layer assigns the model to an existing serving pod. 
In the serving pod a modelmesh-runtime-adapter retrieves the specified model and hands it over to the model adapter to serve.
Once the model is available the modelmesh-runtime-adapter instructs the routing later to forward any requests related to the model to the serving endpoint.
The modelmesh routing layer handles the dynamic nature of inferencing requests in that it can spin up additional model instances when load increase as well 
as reducing the number of serving instances when load decreases. 

The ModelMesh approach is useful for when customers are operating at a large scale with significant numbers of frequently changing models of different types. 

We have covered *ModelMesh* previously so we'll just concentrate on KServe Serving here.

== KServe Serving

KServe Serving takes a different approach to ModelMesh. Using this approach a single model (hence the name) is served from a pod 
and those pods are managed and controlled via OpenShift Serverless (KNative) and ingress traffic is handled by OpenShift Service Mesh. 
Using KNative enables serving pods to be spun up and down, even to zero, depending on request load. Similar to ModelMesh the models are loaded from S3 buckets.


image::Kserve-Serving.png[KServe Architecture]
ref: https://kserve.github.io/website/master/images/kserve_layer.png


At the time of writing the Red Hat OpenShift AI (2.8.0) has the following KServe Serving Runtimes:

* OpenVINO Model Server which can serve the following model formats:
    ** OpenVINO IR
    ** ONNX
    ** Tensorflow
* Caikit TGIS
* TGIS Standalone
* Custom Runtimes (not supported by Red Hat)

[sidebar]
.ModelMesh - KServe Serving feature parity
****
At the time of writing the Red Hat OpenShift AI (2.8.0) KServe does not have full feature parity with ModelMesh.

* KServe uses self-signed certificates out of the box.
* KServe only has public, unauthenticated endpoints.
* KServe metrics are not visable in the RHOAI UI.

All of these features are bring worked on and will be added in a future release of RHOAI.
****

=== KServe Serving example
Similar to ModelMesh, KServe serving utilises the _servingruntimes_ and _inferenceservice_ custom resources.
The following example shows how to use CRs to serve our existing Iris model.

For Serving we will use the OpenVINO serving runtime. 

[TIP] 
Notice that the _multiModel_ attribute is set to *False*, this indicates that the runtime is KServe and not ModelMesh.

```
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: ""
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/template-display-name: OpenVINO Model Server
    opendatahub.io/template-name: kserve-ovms
    openshift.io/display-name: irisv
  labels:
    opendatahub.io/dashboard: "true"
  name: irisv
spec:
  annotations:
    prometheus.kserve.io/path: /metrics
    prometheus.kserve.io/port: "8888"
  containers:
  - args:
    - --model_name={{.Name}}
    - --port=8001
    - --rest_port=8888
    - --model_path=/mnt/models
    - --file_system_poll_wait_seconds=0
    - --grpc_bind_address=127.0.0.1
    - --rest_bind_address=127.0.0.1
    - --target_device=AUTO
    - --metrics_enable
    image: quay.io/modh/openvino_model_server@sha256:5d04d405526ea4ce5b807d0cd199ccf7f71bab1228907c091e975efa770a4908
    name: kserve-container
    ports:
    - containerPort: 8888
      protocol: TCP
    resources:
      limits:
        cpu: "2"
        memory: 8Gi
      requests:
        cpu: "1"
        memory: 4Gi
    volumeMounts:
    - mountPath: /dev/shm
      name: shm
  multiModel: false
  protocolVersions:
  - v2
  - grpc-v2
  supportedModelFormats:
  - autoSelect: true
    name: openvino_ir
    version: opset13
  - name: onnx
    version: "1"
  - autoSelect: true
    name: tensorflow
    version: "1"
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 2Gi
    name: shm
```

The _inferenceservice_ is more interesting as it allows users to specify the Min and Max replica count as well as the S3 location of the model.

```
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: irisv
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
  labels:
    opendatahub.io/dashboard: "true"
  name: irisv
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 0
    model:
      modelFormat:
        name: onnx
        version: "1"
      name: ""
      resources: {}
      runtime: irisv
      storage:
        key: aws-connection-minio
        path: iris
```

[NOTE] 
The _maxReplicas_ and _minReplicas_ enable the serving pods to scale based on traffic. In particular setting the _minReplicas_ to *"0"* will result in the serving pod being terminated after a timeout where no inferencing requests have been received.

[IMPORTANT]
KServe accesses models using versions. However it requires that models are stored in a specific S3 directory structure which is _bucket_name/model_name/model_version_ e.g. *model-data-bucket/iris2/6* . In the InferenceService only the model name field is required e.g. "iris2".

[CAUTION] 
If a serving pod is restarted then the highest version of the model stored on s3 will be retrieved and will be served.

[sidebar]
.Useful HTTP Requests (V2)
****
*To retrieve the metadata of the model served*
```
$ curl https://irisv2-testproject1.apps.example.com/v2/models/irisv2
```
The response should look similar to the following:
```
{"name":"irisv2","versions":["7"],"platform":"OpenVINO","inputs":[{"name":"X","datatype":"FP32","shape":[-1,4]}],"outputs":[{"name":"label","datatype":"INT64","shape":[-1]},{"name":"scores","datatype":"FP32","shape":[-1,3]}]}
```

*To send an inference request* 
```
curl   https://irisv2-testproject1.apps.example.com/v2/models/irisv2/versions/7/infer -X POST --data '{"inputs" : [{"name" : "X","shape" : [ 1, 4 ],"datatype" : "FP32","data" : [ 3, 4, 3, 2 ]}],"outputs" : [{"name" : "output9"}]}'
```

The response should look similar to the following:
```
{
    "model_name": "irisv2",
    "model_version": "7",
    "outputs": [{
            "name": "label",
            "shape": [1],
            "datatype": "INT64",
            "data": [1]
        }, {
            "name": "scores",
            "shape": [1, 3],
            "datatype": "FP32",
            "data": [3.4445483684539797, 3.1545653343200685, 4.803158760070801]
        }]
}
```

*To determine the server version*
```
curl https://irisv2-testproject1.apps.example.com/v2
```

```
{"name":"OpenVINO Model Server","version":"2023.3.0"}
```
****


=== KServe Request/Response Logging

KServe can log request/response payloads and emit _CloudEvents_, these can be consumed by KNative services and/or brokers for further distribution or processing. 

Adding logging is done by modifying the InferenceServce to specify what to log and where to send the Cloud Events e.g.


    logger:
      mode: all
      url: http://cloud-event-receiver.a.b.c/


* The URL is the address of the endpoint to send the cloudevent to
* The mode is the scope, "all" for request/response or "request" or "response"

==== Working Example

First we need to setup a destination for the CloudEvents to be sent to. The following is a simple KNative service just writes the contents of the received CloudEvent to STDOUT. 

```
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: message-logger
spec:
  template:
    spec:
      containers:
      - image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display
```

Now modify the InferenceService to add the logging configuration.

```
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: irisv
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
  labels:
    opendatahub.io/dashboard: "true"
  name: irisv
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 0
    logger:
      mode: all
      url: 'http://message-logger.testproject1.svc.cluster.local'
    model:
      modelFormat:
        name: onnx
        version: "1"
      name: ""
      resources: {}
      runtime: irisv
      storage:
        key: aws-connection-minio
        path: iris
```

When the inference service gets called it will send a CloudEvent to the _message-logger_ pod created by Knative and the contents will be displayed e.g.

```
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   datacontenttype: application/x-www-form-urlencoded
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container Extensions,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   component: predictor
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   endpoint: 
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   inferenceservicename: irisv2
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   namespace: testproject1
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   traceparent: 00-d8850f990eb31ce4d9fc44a7d0b92e49-a7ddee23892e4486-00
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container Data,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   {"inputs" : [{"name" : "X","shape" : [ 1, 4 ],"datatype" : "FP32","data" : [ 3, 4, 3, 8 ]}],"outputs" : [{"name" : "output9"}]}
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container ☁️  cloudevents.Event
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container Validation: valid
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container Context Attributes,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   specversion: 1.0
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   type: org.kubeflow.serving.inference.response
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   source: http://localhost:9081/
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   id: 7535dff2-0ce2-4fc4-9803-22592f57dc1e
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   time: 2024-04-04T15:41:52.783409837Z
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   datacontenttype: application/json
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container Extensions,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   component: predictor
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   endpoint: 
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   inferenceservicename: irisv2
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   namespace: testproject1
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   traceparent: 00-9b1cc0d3b9152925d165e5b50a67f222-11fb14da94923e01-00
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container Data,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   {
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container     "model_name": "irisv2",
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container     "model_version": "7",
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container     "outputs": [
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container       {
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "name": "label",
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "shape": [q
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container           1
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         ],
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "datatype": "INT64",
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "data": [
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container           0
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         ]
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container       },
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container       {
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "name": "scores",
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "shape": [
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container           1,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container           3
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         ],
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "datatype": "FP32",
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         "data": [
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container           7.399900436401367,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container           8.187074661254883,
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container           7.640551567077637
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container         ]
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container       }
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container     ]
message-logger-00001-deployment-84b8dfdcd7-zvbpm user-container   }
```

[NOTE]
The combination of KServe and KNative is a powerful combination enabling the efficient usage of cluster resources by adopting a serverless approach i.e. only creating pods when necessary and spinning them down when not needed.


[sidebar]
.Inference Protocol Versions
****
Currently KServe supports two versions of the Inference Protocol. 

- V1 Inference Protocol.

The V1 protocol can still be used but will be deprecated in the future. For more information see https://kserve.github.io/website/latest/modelserving/data_plane/v1_protocol/[KServe V1 Protocol]

- Open Inference Protocol aka KServe V2 Inference Protocol. 

The https://github.com/kserve/open-inference-protocol[Open Inference Protocol] is focused on passing Tensor values.

It is supported by a number of runtimes:

- NVidia Triton

- MLServer 

- OpenVINO

- TorchServe

- AMD Inference Server

The Open Inference working group is actively working on extending the V2 protocol to enable to be more schema flexible. 
The intention is to deprecate the V1 protocols once this is complete. See https://kserve.github.io/website/latest/modelserving/data_plane/v2_protocol/[KServe V2 protocol] for more information.


The recommendation is to move to/adopt the V2 protocol where possible.

****
=== Transformers

A _Transformer_ is a component that wraps around the Model Inference call in order to perform pre-processing on the incoming Inference request before it's passed to the model for inferencing. A subsequent post-processing step can manipulate the response before it's passed back to the caller. 

The _Transformer_ interface uses the same format as the _Inferencing_ request see https://kserve.github.io/website/0.11/modelserving/data_plane/v2_protocol/[Open Inference Protocol (V2 Inference Protocol)] and both REST and gRPC protocols are supported.

Transformers are built in Python by extending the _Kserve.Model_ class and implementing the *preprocess* and *postprocess* methods.

Transformers can be useful in multiple use cases e.g. 

* Feature store integration.
* Request parameter validation.
* Results processing.

[NOTE]
Transformers support the _V1_ and _V2_ inferencing protocols. Ensure that you are using the correct one based on the model server that you're using.

The KServe repo has a number of Transformer examples e.g.

* https://github.com/kserve/kserve/blob/master/docs/samples/kafka/image_transformer/image_transformer.py[Image Transformer]
* https://github.com/kserve/website/tree/main/docs/modelserving/v1beta1/transformer/torchserve_image_transformer[TorchServer Transformer]
* https://kserve.github.io/website/0.12/modelserving/v1beta1/transformer/feast/[Fast Integration with Transformers]


=== Inference Graph
In a number of advanced use cases AI multiple models often have to be called in sequence to form a complete solution. 
The result of an inference call to one model is fed into the inference input to the next model in the chain.

An example of this in action would be having one model perform feature/anomoly detection and have a second model suggest compensating activities to be performed.


Using multiple models can simplify model training as each model in the chain is only focused on a single aspect of the overall chain.
This is similar to the https://en.wikipedia.org/wiki/Single_responsibility_principle[Single Responsibility Principal] in software engineering.

For more information and examples please look at the https://kserve.github.io/website/latest/modelserving/inference_graph/[Kserve Inference Graph] documentation.
