= Model Serving in RHODS

In this chapter, we will describe the concepts and terminology around model serving on RHODS.

Machine learning models must be deployed in a production environment to process real-time data and handle the problem they were designed to solve.

Deploying a model in a production environment means that the model that has been trained, and exported to a model file format, needs to be imported in a runtime engine, and exposed for applications to consume.

Consume from a model, means that software applications will use a communication method, often REST/HTTP to send a prompt request to a server, such server will fire a request to the model and provide a response. It is evident that the server processing the request, and providing a response based on the model needs to have access to such model.

image::deploy_meaning.drawio.svg[]

Here are some of the concepts that we will explain in this chapter:

. Model Server
. Serving Runtime
. Model server authentication
. GPU Support for model serving
. Model serving formats