= Section 1

== The model

A machine learning model is a program that can learn and make predictions or decisions based on data. It's a fundamental component of machine learning, a subset of artificial intelligence. Machine learning models are designed to automatically improve their performance on a task through experience (i.e., training data) without being explicitly programmed for that task.

For example, in natural language processing, machine learning models can parse and correctly recognize the intent behind previously unheard sentences or combinations of words. In image recognition, a machine learning model can be taught to recognize objects -- such as cars or dogs. A machine learning model can perform such tasks by having it __trained__ with a large dataset. During training, the machine learning algorithm is optimized to find certain patterns or outputs from the dataset, depending on the task. The output of this process -- often a computer program with specific rules and data structures -- is called a machine learning model.

The process of creating a model includes steps for data collection and selection, the selection of an algorithm to process the data, training the model, validating it, and exporting it so that it can be deployed in a later step.

image::ml_workflow.drawio.svg[Machine Learning Workflow]

== Train a model

Using a RHODS instance, let us train and deploy an example.

. In a data science project, create a `Standard Data Science`workbench.
Then, open the workbench to go to the JuypterLab interface.
+
image::workbench_options.png[Workbench Options]

. Make sure that the workbench environment serves the required python packages for the notebook to run, for this to happen, open a terminal and run the following command to verify that the packages are already installed:
+
```shell
  pip install numpy matplotlib scikit-learn
```
+
image::terminal-install.png[Install packages]

. In **JupyterLab**, create a new notebook, and name it **purchase-amount**.
+
image::purchase-amount-notebook.png[purchase-amount notebook]

. Use the following code to train and export a model that predicts the amount of a pruchase given the number of minutes to make such purchase in a website.
+
```python
# Include required packages for execution
import numpy
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
import pickle

# Create data
numpy.random.seed(2)
# Create a random set of numbers for a mock representation of minutes spent online
x = numpy.random.normal(3,1,100)
# Create a random set of number for a mock representation of the purchase amount
y = numpy.random.normal(150,40,100) / x

# Divide the data between tranin and test
# where the training set is the 80% of the data
# while the test set is the remaining 20%
train_x = x[:80]
train_y = y[:80]

test_x = x[80:]
test_y = y[80:]

# train a model using polynomial regression
# The number 4 here represents a hyperparameter
mymodel = numpy.poly1d(numpy.polyfit(train_x, train_y, 4))
# export the model to a file
filename = 'mymodel.pkl'
pickle.dump(mymodel, open(filename, 'wb'))
```

. Notice the creation of a new file in your environment, the `mymodel.pkl`
+
image::mymodel-pkl.png[Model file export]

[IMPORTANT]
====
There are different formats and libraries to export the model, in this case we are using pickle. Other common formats are:

* Protobuf

* MLeap

* H5

* ONNX

* PMML

* Torch

The use of either of those formats depend on the target server runtime, some of them are proven to be more eficient than others for certain type of training algorithms and model sizes.
====

== Use the model in another notebook

The model can be deserialized in another notebook, and used to generate a prediction:

. Create a notebook called **use-purchase-amount**
+
image::use-purchase-amount-notebook.png[use-purchase-amount notebook create]

. Use the following code to instantiate the model and request for a prediction
+
```python
# Include required packages for execution
import pickle

# Load models
with open('mymodel.pkl', 'rb') as f:
    model = pickle.load(f)
  
# perform a prediction using the model
print(model(4))
```
+
image::prediction-with-model.png[prediction with model]

[TIP]
====
At this moment the model can be exported and imported in other projects for its use. Normally there will be an S3 bucket or a model registry to store models and versions of such models, and instead of manually exporting the model, there would be pipelines making the model available.
====

== Use the Model in a Container

For this section, you will need postman (or docker) to create an image, and a registry to upload the resulting image.

=== web application that uses the model

The pickle model that we previously exported can be used in a Flask application. In this section we present an example Flask application that uses the model.

[IMPORTANT]
====
Although we are actually serving a model with Flask in the exercise, Flask is not considered part of the Model Serving feature. This example represents one way in which some customers decide to embed their models in containers, although RHODS provides for mechanisms that can make this process of serving a model a simpler process, when provided with the proper model formats.
====

. In your computer, create a new directory to save the source code of the web application.
Navigate to that directory.

. Download the `mymodel.pkl` file from JupyterLab into this directory.

. Open the directory with a python IDE, then create a python script named `app.py` with the following code:
+
```python[app.py]
from flask import Flask, request
import pickle

app = Flask(__name__)
# Load model
with open('mymodel.pkl', 'rb') as f:
    model = pickle.load(f)

model_name = "Time to purchase amount predictor"
model_file = 'model.plk'
version = "v1.0.0"


@app.route('/info', methods=['GET'])
def info():
    """Return model information, version how to call"""
    result = {}

    result["name"] = model_name
    result["version"] = version

    return result


@app.route('/health', methods=['GET'])
def health():
    """REturn service health"""
    return 'ok'


@app.route('/predict', methods=['POST'])
def predict():
    feature_dict = request.get_json()
    if not feature_dict:
        return {
            'error': 'Body is empty.'
        }, 500

    try:
        return {
            'status': 200, 
            'prediction': int(model(feature_dict['time']))
        }
    except ValueError as e:
        return {'error': str(e).split('\n')[-1].strip()}, 500


if __name__ == '__main__':
    app.run(host='0.0.0.0')
```

. Create a `requirements.txt` to describe the python dependencies to install on container startup:
+
```[requirements.txt]
click==8.0.3
cycler==0.11.0
Flask==2.0.2
fonttools==4.28.5
gunicorn==20.1.0
itsdangerous==2.0.1
Jinja2==3.0.3
kiwisolver==1.3.2
MarkupSafe==2.0.1
matplotlib==3.5.1
numpy==1.22.0
packaging==21.3
pandas==1.3.5
Pillow==9.0.0
pyparsing==3.0.6
python-dateutil==2.8.2
pytz==2021.3
scikit-learn==1.0.2
scipy==1.7.3
six==1.16.0
sklearn==0.0
threadpoolctl==3.0.0
Werkzeug==2.0.2
```

. Create a `Containerfile` to build an image with the Flask application:
+
```docker[containerfile]
# Base image
FROM python:3.9

# Set working directory
WORKDIR /app

# Copy files
COPY app.py /app <1>
COPY requirements.txt /app <2>
COPY mymodel.pkl /app <3>

# Install dependencies
RUN pip install -r requirements.txt

# Run the application
EXPOSE 8000
ENTRYPOINT ["gunicorn", "-b", "0.0.0.0:8000", "--access-logfile", "-", "--error-logfile", "-", "--timeout", "120"]
CMD ["app:app"]
```
<1> The python application source code
<2> The list of packages to install
<3> The model

. Build and push the image to an image registry
+
```shell
podman login quay.io
podman build -t purchase-predictor:1.0 .
podman tag purchase-predictor:1.0 quay.io/user_name/purchase-predictor:1.0
podman push quay.io/user_name/purchase-predictor:1.0
```

. Deploy the model image to **OpenShift**
+
```shell
oc login api.cluster.example.com:6443
oc new-project model-deploy
oc new-app --name purchase-predictor quay.io/user_name/purchase-predictor:1.0
oc expose service purchase-predictor
```

Now we can use the Flask application with some commands such as:
```shell
curl http://purchase-predictor-model-deploy.apps.cluster.example.com/health
ok%
curl http://purchase-predictor-model-deploy.apps.cluster.example.com/info
{"name":"Time to purchase amount predictor","version":"v1.0.0"}
curl -d '{"time":4}' -H "Content-Type: application/json" -X POST http://purchase-predictor-model-deploy.apps.cluster.example.com/predict
{"prediction":34,"status":200}
```

[IMPORTANT]
====
In this section we have manually:

. Developed an application that uses the model

. Built an image with such application

. Push the image to a registry

. Deployed the containerized application in OpenShift

. Exposed the application's endpoint in OpenShift by creating a route

. Consumed the model through the application's REST API to request a prediction

There are automated and faster ways to perform these steps. In the following sections, we will learn about runtimes that only require you to provide a model, and they automatically provision an inference service for you.
====