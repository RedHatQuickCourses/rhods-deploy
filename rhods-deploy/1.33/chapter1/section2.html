<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenVINO Model Serving :: Deploying Machine Learning Models with Red Hat OpenShift AI</title>
    <link rel="prev" href="section1.html">
    <link rel="next" href="section3.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Deploying Machine Learning Models with Red Hat OpenShift AI</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/rhods-deploy/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="rhods-deploy" data-version="1.33">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Deploying Machine Learning Models with Red Hat OpenShift AI</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Home</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Model Serving in Red Hat OpenShift AI</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section1.html">Model Serving Concepts</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="section2.html">OpenVINO Model Serving</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="section3.html">Creating a Custom Model Serving Runtime</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Deploying Machine Learning Models with Red Hat OpenShift AI</span>
    <span class="version">1.33</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Deploying Machine Learning Models with Red Hat OpenShift AI</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1.33</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Deploying Machine Learning Models with Red Hat OpenShift AI</a></li>
    <li><a href="index.html">Model Serving in Red Hat OpenShift AI</a></li>
    <li><a href="section2.html">OpenVINO Model Serving</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">OpenVINO Model Serving</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In this section we will work in an exercise to deploy a model to an OpenVINO Serving Runtime.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prepare_minio"><a class="anchor" href="#_prepare_minio"></a>Prepare MinIO</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://min.io">MinIO</a> is a high-performance, S3 compatible object store. It is built for large scale AI/ML, data lake and database workloads. It is software-defined and runs on any cloud or on-premises infrastructure.</p>
</div>
<div class="paragraph">
<p>We will need an S3 solution to share the model from training to deploy, in this exercise we will prepare MinIO to be such S3 solution.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In OpenShift, create a new namespace with the name <strong>object-datastore</strong>.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ oc new-project object-datastore</code></pre>
</div>
</div>
</li>
<li>
<p>Run the following yaml to install MinIO:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ oc apply -f https://raw.githubusercontent.com/RedHatQuickCourses/rhods-qc-apps/main/4.rhods-deploy/chapter2/minio.yml -n object-datastore</code></pre>
</div>
</div>
</li>
<li>
<p>Get the route to the MinIO dashboard.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ oc get routes -n object-datastore | grep minio-ui | awk '{print $2}'</code></pre>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>Use this route to navigate to the S3 dashboard using a browser. With the browser, you will be able to create buckets, upload files, and navigate the S3 contents.</p>
</div>
</div>
</div>
</li>
<li>
<p>Get the route to the MinIO API.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ oc get routes -n object-datastore | grep minio-api | awk '{print $2}'</code></pre>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>Use this route as the S3 API endpoint. Basically, this is the URL that we will use when creating a data connection to the S3 in RHOAI.</p>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_training_the_model"><a class="anchor" href="#_training_the_model"></a>Training The Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will use the iris dataset model for this excercise.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Using a JupyterLab workbench at RHOAI, import the repository: <a href="https://github.com/RedHatQuickCourses/rhods-qc-apps.git" class="bare">https://github.com/RedHatQuickCourses/rhods-qc-apps.git</a></p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It is recommended to use a workbench that was created with the <strong>Standard Data Science</strong> Notebook image.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Make sure that the workbench environment serves the required python packages for the notebook to run, for this to happen, open a terminal and run the following command to verify that the packages are already installed:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ pip install -r /opt/app-root/src/rhods-qc-apps/4.rhods-deploy/chapter2/requirements.txt</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You might also want to execute the preceding command in the notebook kernel by using the <code>%pip</code> syntax in the notebook.
Alternatively, you can create a custom notebook image that includes the <code>skl2onnx</code> package.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open and run the notebook <strong>iris_to_onnx</strong> from <strong>rhods-qc-apps/4.rhods-deploy/chapter2</strong> directory</p>
<div class="imageblock">
<div class="content">
<img src="_images/iris_training_onnx.png" alt="iris training to onnx format">
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Converting a model to ONNX format depends on the library that you use to create the model.
In this case, the model is created with Scikit-Learn, so you must use the <a href="https://onnx.ai/sklearn-onnx/">sklearn-onnx</a> library to perform the conversion.</p>
</div>
<div class="paragraph">
<p>To convert from PyTorch, see <a href="https://pytorch.org/tutorials/beginner/onnx/intro_onnx.html">Introduction to ONNX in the PyTorch docs</a>.</p>
</div>
<div class="paragraph">
<p>To convert from TensorFlow, use the <a href="https://github.com/onnx/tensorflow-onnx">tf2onnx</a> library.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Observe that a file has been created: <code>rf_iris.onnx</code>, download this file to your computer, so that we can upload it to S3.</p>
<div class="imageblock">
<div class="content">
<img src="_images/iris-download.png" alt="iris model download">
</div>
</div>
</li>
<li>
<p>Upload the file <code>rf_iris.onnx</code> to a bucket named <strong>models</strong>, with a path <strong>iris</strong> in your S3. The username is <strong>minio</strong> and the password is <strong>minio123</strong>.</p>
<div class="imageblock">
<div class="content">
<img src="_images/iris-s3-upload.png" alt="iris model s3 upload">
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Make sure to create a new path in your bucket, and upload to such path, not to root. Later, when requesting to deploy a model to the <strong>Model Server</strong>, you will be required to provide a path inside your bucket.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_a_data_connection"><a class="anchor" href="#_create_a_data_connection"></a>Create A Data Connection</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the RHOAI dashboard, create a project named <strong>iris-project</strong>.</p>
</li>
<li>
<p>In the <strong>Data Connections</strong> section, create a Data Connection to your S3.</p>
<div class="imageblock">
<div class="content">
<img src="_images/add-minio-iris-data-connection.png" alt="Add iris data connection from minio">
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>The credentials (Access Key/Secret Key) are <code>minio</code>/<code>minio123</code>.</p>
</li>
<li>
<p>Make sure to use the API route, not the UI route (<code>oc get routes -n object-datastore | grep minio-api | awk '{print $2}'</code>).</p>
</li>
<li>
<p>The region is not important when using MinIO, this is a property that has effects when using AWS S3.
However, you must enter a non-empty value to prevent problems with model serving.</p>
</li>
<li>
<p>Mind typos for the bucket name.</p>
</li>
<li>
<p>You don&#8217;t have to select a workbench to attach this data connection to.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_boto3"><a class="anchor" href="#_using_boto3"></a>Using <code>boto3</code></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Although the previous section indicates that you should manually download the <code>rf_iris.onnx</code> file to your computer and upload it to S3, you can also upload your model directly from your notebook or Python file, by using the <code>boto3</code> library.
To use this approach, you must:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Have the <code>boto3</code> library installed in your workbench (most of the RHOAI notebook images include this library).</p>
</li>
<li>
<p>Attach your data connection to the workbench.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>After training the model, you can upload the file as the following example demostrates:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import os
import boto3

source_path = "model.onnx"
s3_destination_path = "models/model.onnx"

key_id = os.getenv("AWS_ACCESS_KEY_ID")
secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
endpoint = os.getenv("AWS_S3_ENDPOINT")
bucket_name = os.getenv("AWS_S3_BUCKET")

s3 = boto3.client(
   "s3",
   aws_access_key_id=key_id,
   aws_secret_access_key=secret_key,
   endpoint_url=endpoint,
   use_ssl=True)

s3.upload_file(source_path, bucket_name, Key=s3_destination_path)</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can also use the <code>boto3</code> library to download data.
This can be helpful in the data collection stage, for example for gathering data files from S3.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">s3_data_path = "dataset.csv"
s3.download_file(bucket_name, s3_data_path, "my/local/path/dataset.csv")</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_a_model_server"><a class="anchor" href="#_create_a_model_server"></a>Create a Model Server</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the <strong>Models and model servers</strong> section, add a server.</p>
<div class="imageblock">
<div class="content">
<img src="_images/add-server-button.png" alt="add server">
</div>
</div>
</li>
<li>
<p>Fill the form with the following values:</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>Server name: <code>iris-model-server</code>.</p>
</li>
<li>
<p>Serving runtime: <code>OpenVINO Model Server</code>.</p>
</li>
<li>
<p>Select the checkboxes to expose the models through an external route, and to enable token authentication.
Enter <code>iris-serviceaccount</code> as the service account name.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/add-server-form-example.png" alt="Add Server Form">
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The model server you are creating works as a template for deploying models. As you can see, we have not specified the model that we will deploy, or the data connection from where that model will be retrieved, in this form we are specifying the resources, constraints, and engine that will define the engine where the model will be deployed later.
It is important to pay special attention to the following characteristics:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Serving Runtime</strong>: By default we have <em>OpenVINO</em> and <em>OpenVINO with GPU</em>. The important aspects when defining these runtimes are: The framework that is capable of reading models in a given format, and weather such platform supports using GPUs. The use of GPUs allow for complex and lengthy computations to be delivered faster, as there are huge models that require a good amount of power to calculate, based on the given parameters a prediction.</p>
</li>
<li>
<p><strong>Number of replicas to deploy</strong>: Planning for expected performance and number of expected requests is essential for this part of the form. Here we select if we will load balance a given request between multiple container replicas.</p>
</li>
<li>
<p><strong>Model Server Size</strong>: In this part of the form we define the resources assigned to each model server container. You can create and select a pre-defined size from the dropdown, or you can select <em>custom</em>, in which case, new fields will be displayed to request the processing and memory power to be assigned to your containers.</p>
<div class="imageblock">
<div class="content">
<img src="_images/model-server-size.png" alt="model server size">
</div>
</div>
</li>
<li>
<p><strong>Model Route</strong>: There are models that can be consumed only from other containers inside the same OpenShift cluster, here we have the ability to not make this server available to entities outside our cluster, or to instruct the model server configuration to assign an external route. When we don&#8217;t expose the model externally through a route, click on the Internal Service link in the Inference endpoint section:</p>
<div class="imageblock">
<div class="content">
<img src="_images/figure14_0.png" alt="Inference endpoint">
</div>
</div>
<div class="paragraph">
<p>A popup will display the address for the gRPC and the REST URLs:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/figure15_0.png" alt="Endpoint URLs">
</div>
</div>
</li>
<li>
<p><strong>Token authorization</strong>: In this part of the form we have a helper checkmark to add authorization to a service account that will be created with access to our model server. Only API requests that present a token that has access to the given service account will be able to run the inference service.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>After clicking the <strong>Add</strong> button at the bottom of the form, you will be able to see a new <strong>Model Server</strong> configuration in your project, you can click the <strong>Tokens</strong> column, which will make visible the tokens that you can share with the applications that will consume the inference API.</p>
<div class="imageblock">
<div class="content">
<img src="_images/model-server-with-token.png" alt="Model Server with token">
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploy_the_model"><a class="anchor" href="#_deploy_the_model"></a>Deploy The Model</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>At the right side of the <strong>Model Server</strong>, we can find the <strong>Deploy Model</strong> button, let&#8217;s click the <strong>Deploy Model</strong> button, to start filling the <strong>Deploy Model</strong> form:</p>
<div class="imageblock">
<div class="content">
<img src="_images/deploy-model-button.png" alt="Deploy Model button">
</div>
</div>
</li>
<li>
<p>Fill the <strong>Deploy Model</strong> form.</p>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>Model name: <code>iris-model</code></p>
</li>
<li>
<p>Model framework: <code>onnx - 1</code></p>
</li>
<li>
<p>Model location data connection: <code>iris-data-connection</code></p>
</li>
<li>
<p>Model location path: <code>iris</code></p>
</li>
</ul>
</div>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/deploy-model-form.png" alt="Deploy Model form">
</div>
</div>
</li>
<li>
<p>After clicking the <strong>Add</strong> button at the bottom of the form, you will be able to see a new entry at the <strong>Deployed models</strong> column for your <strong>Model Server</strong>, clicking in the column will eventually show a check mark under the <strong>Status</strong> column:</p>
<div class="imageblock">
<div class="content">
<img src="_images/deploy-model-success.png" alt="Deploy model success">
</div>
</div>
</li>
<li>
<p>Observe and monitor the assets created in your OpenShift <strong>iris-project</strong> namespace.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ oc get routes -n iris-project
$ oc get secrets -n iris-project | grep iris-model
$ oc get events -n iris-project</code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/iris-project-events.png" alt="Iris project events">
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Deploying a <strong>Model Server</strong> triggers a <strong>ReplicaSet</strong> with <strong>ModelMesh</strong>, which attach your model to the inference runtime, and exposes it through a route. Also, notice the creation of a secret with your token.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_test_the_model"><a class="anchor" href="#_test_the_model"></a>Test The Model</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now that the model is ready to use, we can make an inference using the REST API.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Assign the route to an environment variable in your local machine, so that we can use it in our curl commands.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ export IRIS_ROUTE=https://$(oc get routes -n iris-project | grep iris-model | awk '{print $2}')</code></pre>
</div>
</div>
</li>
<li>
<p>Assign an authentication token to an environment variable in your local machine.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ export TOKEN=$(oc whoami -t)</code></pre>
</div>
</div>
</li>
<li>
<p>Request an inference with the REST API.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-console hljs" data-lang="console">$ curl -H "Authorization: Bearer $TOKEN" $IRIS_ROUTE/v2/models/iris-model/infer \
 -X POST \
 --data '{"inputs" : [{"name" : "X","shape" : [ 1, 4 ],"datatype" : "FP32","data" : [ 3, 4, 3, 2 ]}],"outputs" : [{"name" : "output0"}]}'</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>The result of using the inference service looks like the following output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{"model_name":"iris-model__isvc-590b5324f9","model_version":"1","outputs":[{"name":"label","datatype":"INT64","shape":[1],"data":[1]},{"name":"scores","datatype":"FP32","shape":[1,3],"data":[4.851966,3.1275764,3.4580243]}]}</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_model_serving_request_body"><a class="anchor" href="#_model_serving_request_body"></a>Model Serving Request Body</h3>
<div class="paragraph">
<p>As you tested with the preceding <code>curl</code> command, to make HTTP requests to a deployed model you must use a specific request body format.
The basic format of the input data is as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>{
  "inputs": [{
    "name" : "input", <i class="conum" data-value="1"></i><b>(1)</b>
    "shape" : [2,3], <i class="conum" data-value="2"></i><b>(2)</b>
    "datatype"  : "INT64", <i class="conum" data-value="3"></i><b>(3)</b>
    "data" : [[34, 54, 65], [4, 12, 21]] <i class="conum" data-value="4"></i><b>(4)</b>
  }]
}</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The name of the input tensor.
The data scientist that creates the model must provide you with this value.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The shape of the input tensor.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The <a href="https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#tensor-data-types">data type</a> of the input tensor.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>The tensor contents provided as a JSON array.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The API supports additional parameters.
For a complete list, refer to the <a href="https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference-request-json-object">Kserve Predict Protocol docs</a>.</p>
</div>
<div class="paragraph">
<p>To make a request in Python, you can use the <code>requests</code> library, as the following example shows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import requests

input_data = [-0.15384616, -0.9909186]

# You must adjust this path or read it from an environment variable
INFERENCE_ENDPOINT = "https://my-model.apps.my-cluster.example.com/v2/models/my-model/infer"

# Build the request body
payload = {
    "inputs": [
        {
            "name": "dense_input",
            "shape": [1, 2],
            "datatype": "FP32",
            "data": input_data
        }
    ]
}

# Send the POST request
response = requests.post(INFERENCE_ENDPOINT, json=payload)

# Parse the JSON response
result = response.json()

# Print predicted values
print(result['outputs'][0]['data'])</code></pre>
</div>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="section1.html">Model Serving Concepts</a></span>
  <span class="next"><a href="section3.html">Creating a Custom Model Serving Runtime</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
